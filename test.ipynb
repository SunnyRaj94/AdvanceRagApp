{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98e651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/sunnyraj/code_files/git_repos/AdvanceRagApp')\n",
    "\n",
    "from backend.llm_engine import get_response_from_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7307519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLMs, RAG stands for \"Risk, Analysis, and Governance.\" It is a framework used to assess and manage risks associated with large-scale machine learning models. The purpose of RAG is to provide a structured approach for identifying, evaluating, and mitigating risks that may arise from the use of these models.\n",
      "\n",
      "The RAG framework typically involves three stages:\n",
      "\n",
      "1. Risk: Identifying potential risks associated with the model, such as bias, data privacy, security, ethical concerns, and legal implications.\n",
      "2. Analysis: Evaluating the likelihood and potential impact of each identified risk. This includes assessing the model's performance, its interaction with the data and users, and any potential consequences of its decisions.\n",
      "3. Governance: Establishing policies, procedures, and controls to mitigate risks and ensure that the model is used responsibly and ethically. This includes implementing monitoring and auditing mechanisms, as well as providing transparency and accountability.\n",
      "\n",
      "The RAG framework is designed to help organizations manage the complex risks associated with large-scale machine learning models and ensure that these models are used in a responsible and ethical manner.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What is the purpose of RAG in LLMs? [/INST]\"\n",
    "\n",
    "print(get_response_from_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67b5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunnyraj/miniconda3/envs/rag_app/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Ingesting document: https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation\n",
      "âœ… Extracted 26576 characters\n",
      "âœ… Chunked into 60 pieces\n",
      "âœ… Created embeddings of shape (60, 384)\n",
      "âœ… Stored 60 chunks in vector store\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from backend.rag.ingest import ingest_document\n",
    "\n",
    "# From file\n",
    "# ingest_document(\"sample_docs/sample.pdf\")\n",
    "\n",
    "# From web\n",
    "ingest_document(\"https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6550f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_nested_dirs(start_path):\n",
    "    \"\"\"\n",
    "    Generates a list of all directories within start_path, including nested ones.\n",
    "\n",
    "    Args:\n",
    "        start_path (str): The path to the directory to start searching from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full paths to all directories found, including the start_path.\n",
    "              Returns an empty list if start_path is not a valid directory.\n",
    "    \"\"\"\n",
    "    all_dirs = []\n",
    "    # Ensure the starting path is a valid directory\n",
    "    if not os.path.isdir(start_path):\n",
    "        print(f\"Error: Provided path '{start_path}' is not a valid directory or doesn't exist.\")\n",
    "        return all_dirs # Return an empty list\n",
    "\n",
    "    # os.walk() efficiently traverses the directory tree.\n",
    "    # For each directory it visits, it yields a tuple:\n",
    "    # (current_directory_path, list_of_subdirectory_names, list_of_filenames)\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        # The 'dirpath' is exactly what we want: the path to a directory\n",
    "        # found during the walk (including the starting directory itself).\n",
    "        all_dirs.append(dirpath)\n",
    "\n",
    "        # Note: We don't need to explicitly recurse into 'dirnames'.\n",
    "        # os.walk() handles the traversal down the tree automatically.\n",
    "\n",
    "    return all_dirs\n",
    "\n",
    "def get_files_nested(root_dir):\n",
    "    \"\"\"\n",
    "    Returns a list of all file paths within a specified root directory\n",
    "    and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The path to the root directory to start the search from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is the full path to a file.\n",
    "              Returns an empty list if the root directory is not found or\n",
    "              contains no files.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    if os.path.isdir(root_dir):\n",
    "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                file_list.append(os.path.join(dirpath, filename))\n",
    "    else:\n",
    "        print(f\"Error: Directory '{root_dir}' not found.\")\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2423fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'LICENSE',\n",
       " 'requirements.txt',\n",
       " 'README.md',\n",
       " '.gitignore',\n",
       " 'main.py',\n",
       " 'frontend/.DS_Store',\n",
       " 'frontend/ui.py',\n",
       " 'frontend/static/js/app.js',\n",
       " 'frontend/templates/index.html',\n",
       " 'frontend/templates/login.html',\n",
       " 'frontend/templates/upload.html',\n",
       " 'frontend/templates/chat.html',\n",
       " 'test/test.ipynb',\n",
       " 'test/vector_store/index.faiss',\n",
       " 'test/vector_store/meta.pkl',\n",
       " 'config/settings.yaml',\n",
       " 'config/__init__.py',\n",
       " 'backend/auth.py',\n",
       " 'backend/llm_engine.py',\n",
       " 'backend/__init__.py',\n",
       " 'backend/api.py',\n",
       " 'backend/storage.py',\n",
       " 'backend/rag_pipeline.py',\n",
       " 'backend/rag/ingest.py',\n",
       " 'backend/rag/query.py',\n",
       " 'backend/rag/vector_store.py',\n",
       " 'backend/rag/processor.py',\n",
       " 'backend/rag/logger.py',\n",
       " 'backend/rag/embedder.py',\n",
       " 'models/mistral.gguf',\n",
       " 'models/all-MiniLM-L6-v2/model.safetensors',\n",
       " 'models/all-MiniLM-L6-v2/tokenizer_config.json',\n",
       " 'models/all-MiniLM-L6-v2/special_tokens_map.json',\n",
       " 'models/all-MiniLM-L6-v2/config.json',\n",
       " 'models/all-MiniLM-L6-v2/config_sentence_transformers.json',\n",
       " 'models/all-MiniLM-L6-v2/tokenizer.json',\n",
       " 'models/all-MiniLM-L6-v2/rust_model.ot',\n",
       " 'models/all-MiniLM-L6-v2/README.md',\n",
       " 'models/all-MiniLM-L6-v2/train_script.py',\n",
       " 'models/all-MiniLM-L6-v2/sentence_bert_config.json',\n",
       " 'models/all-MiniLM-L6-v2/vocab.txt',\n",
       " 'models/all-MiniLM-L6-v2/.gitattributes',\n",
       " 'models/all-MiniLM-L6-v2/pytorch_model.bin',\n",
       " 'models/all-MiniLM-L6-v2/data_config.json',\n",
       " 'models/all-MiniLM-L6-v2/tf_model.h5',\n",
       " 'models/all-MiniLM-L6-v2/modules.json',\n",
       " 'models/all-MiniLM-L6-v2/1_Pooling/config.json',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_quint8_avx2.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_O2.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_qint8_arm64.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_O3.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_O4.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_qint8_avx512.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_qint8_avx512_vnni.onnx',\n",
       " 'models/all-MiniLM-L6-v2/onnx/model_O1.onnx',\n",
       " 'models/all-MiniLM-L6-v2/openvino/openvino_model_qint8_quantized.xml',\n",
       " 'models/all-MiniLM-L6-v2/openvino/openvino_model_qint8_quantized.bin',\n",
       " 'models/all-MiniLM-L6-v2/openvino/openvino_model.xml',\n",
       " 'models/all-MiniLM-L6-v2/openvino/openvino_model.bin',\n",
       " 'logs/app.log']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_files = get_files_nested('/Users/sunnyraj/code_files/git_repos/AdvanceRagApp')\n",
    "ignore_files = ['/.git/', '__pycache__']\n",
    "final_strc = []\n",
    "for a in nested_files:\n",
    "    include = True\n",
    "    for b in ignore_files:\n",
    "        if b in a:\n",
    "            include = False\n",
    "    if include is True:\n",
    "        final_strc.append(a.replace(\"/Users/sunnyraj/code_files/git_repos/AdvanceRagApp/\",''))\n",
    "final_strc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ccc7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'id': \"ab05bd93-92f2-40dd-8cdc-2560e67d1566\", 'source': \"https://www.w3schools.com/python/\", 'filename': \"ab05bd93-92f2-40dd-8cdc-2560e67d1566\"}\n",
    "\n",
    "query='what is the link to this url and what is this article talking about'\n",
    "top_k=5\n",
    "source='url'\n",
    "file_id=None\n",
    "link_id='ab05bd93-92f2-40dd-8cdc-2560e67d1566'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85319bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/sunnyraj/code_files/git_repos/AdvanceRagApp')\n",
    "\n",
    "from backend.llm_engine import get_response_from_llm\n",
    "from backend.rag.query import query_rag_system\n",
    "\n",
    "\n",
    "response = query_rag_system(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "            source=source,\n",
    "            file_id=file_id,\n",
    "            link_id='5430caa1-1697-4041-8735-cd28c3887bc0',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b4e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nAnswer:\\n\\n\\nThis URL appears to be a news article from the New York Times published on November 18, 2021. The article is titled \"Facebook Says It Will End Most of Its Experiments with Human Subjects\".\\n\\nAccording to the article, Facebook has been conducting experiments on its users for years, often without their full understanding or consent. The social media giant has now announced that it will end most of these experiments, with the exception of a few that are critical to the safety and functioning of the platform.\\n\\nThe article discusses the ethical and legal concerns surrounding human subject research on social media platforms, and highlights the need for greater transparency and accountability. It also notes that the move by Facebook to end most of its experiments is likely to be welcomed by many users who have expressed concerns about the company\\'s practices.\\n\\nOverall, this article is about the decision by Facebook to end most of its experiments with human subjects, and the ethical and legal considerations surrounding such research.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19603dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunnyraj/miniconda3/envs/rag_app/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from backend.rag.embedder import get_embeddings\n",
    "query_embedding = get_embeddings([query])[0]\n",
    "dim = len(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97189f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim#, query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7662fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.rag.vector_store import VectorStore\n",
    "# Step 2: Load vector store\n",
    "vectorstore = VectorStore(dim)\n",
    "vectorstore.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ff243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file', 'url'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([a['source_type'] for a in vectorstore.text_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc0d03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'rag-test/eBook-How-to-Build-a-Career-in-AI.pdf at main Â· NisugaJ/rag-test Â· GitHub\\nSkip to content\\nNavigation Menu\\nToggle navigation\\nSign in\\nProduct\\nGitHub Copilot\\nWrite better code with AI\\nGitHub Advanced Security\\nFind and fix vulnerabilities\\nActions\\nAutomate any workflow\\nCodespaces\\nInstant dev environments\\nIssues\\nPlan and track work\\nCode Review\\nManage code changes\\nDiscussions\\nCollaborate outside of code\\nCode Search\\nFind more, search less\\nExplore\\nWhy GitHub\\nAll features\\nDocumentation\\nGitHub Ski',\n",
       " 'source_type': 'file',\n",
       " 'file_id': None,\n",
       " 'link_id': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "281fcc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to make metadatastore\n",
      "ðŸ“¥ Ingesting source: https://medium.com/@mail-sunnyraj94/web-scraping-guide-78b26c0d8bca (type: url)\n",
      "Preview:\n",
      "Web-Scraping : Guide. Youâ€™ve probably heard multiple timesâ€Šâ€”â€¦ | by Sunnyraj | Medium\n",
      "Open in app\n",
      "Sign up\n",
      "Sign in\n",
      "Write\n",
      "Sign up\n",
      "Sign in\n",
      "Web-Scraping : Guide\n",
      "Sunnyraj\n",
      "Follow\n",
      "15 min read\n",
      "Â·\n",
      "Dec 12, 2021\n",
      "--\n",
      "Listen\n",
      "Share\n",
      "Youâ€™ve probably heard multiple times â€”\n",
      "â€œData is the new oil!â€\n",
      "And yes this is true\n",
      "DATA\n",
      "is the new differentiator. It is the core of market research and business strategies. Whether you want to start a new project or churn out a new strategy for an existing business, you need to invariably access and analyze a vast amount of data.\n",
      "Introduction to Web Scraping\n",
      "Web scraping is a technique to fetch data from websites . One way is to manually copy-paste the needed data from the websites but this is very much time-consuming .Web scrapping automate the process of data extraction from websites which is done with the help of software or programs called scrappers . These can be custom built to work for one site or can be configured to work with any website based on user requirements.\n",
      "âœ… Extracted 21576 characters\n",
      "âœ… Chunked into 48 pieces\n",
      "âœ… Created embeddings of shape (48, 384)\n",
      "âœ… Stored 48 chunks in vector store\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5430caa1-1697-4041-8735-cd28c3887bc0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from backend.rag.ingest import file_metadata_store, ingest_data\n",
    "ingest_data(\n",
    "    'https://medium.com/@mail-sunnyraj94/web-scraping-guide-78b26c0d8bca', \"url\", \"anonymous\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198fd7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "efca8673-c7d5-4e20-848b-d53fb3b576f5\n"
     ]
    }
   ],
   "source": [
    "filter_by = source = 'url'\n",
    "link_id = 'efca8673-c7d5-4e20-848b-d53fb3b576f5'\n",
    "context_id = (\n",
    "    file_id\n",
    "    if source == \"file\" and file_id\n",
    "    else link_id if source == \"url\" and link_id else None\n",
    ")\n",
    "print(\"-----------------------\")\n",
    "print(context_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2c85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "relevant_chunks = vectorstore.search(\n",
    "        np.array([query_embedding]),\n",
    "        top_k=10,\n",
    "        filter_by=filter_by,\n",
    "        context_id=context_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a122f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d click on links. Pretty cool, right?\\nIt also comes with Python bindings for controlling it right from your application. This makes it a breeze to integrate with your chosen parsing library.\\nYou can find Selenium\\nDocumentation\\nHere.\\n# To install â€” pip install\\nselenium\\n# To import â€” import selenium\\nHere are some basic operation example.\\nAdditional Resource :\\nhere are two articles where you can see some more in-depth operations the article is in two parts â€”\\nPART1\\nand\\nPART2\\n.\\n5. Scrapy\\nScrapy is a ',\n",
       " 'te for data scraping as incomplete information regarding the website structure will lead to improper scraping of data.\\nSlowness\\nIf we are scrapping a large number of websites / pages we can come across such problem where scrappers takes a lot of time. It is really important to optimize our scripts to address such problem. We can also use multiple instances or dockers to minimize that.\\nQuality and Freshness of the Data\\nThe quality and freshness of data must be one of your most important criteria ',\n",
       " 'o parts â€”\\nPART1\\nand\\nPART2\\n.\\n5. Scrapy\\nScrapy is a web scraping framework. It is one of the most advanced scraping framework available in Python. This Scrapy provides bots that can scrape thousands of web pages at once. Here you have to create a web spider that will go from one page to another and provides you the data.\\nScrapy is the greatest Web Scraping framework, and it was developed by a team with a lot of enterprise scraping experience. The software created on top of this library can be a cr',\n",
       " 'rome, Remote, etc. A program can do almost all the tasks that can be performed by a user on web browsers like form fillings, form clicking or button clicking, browser opening and a lot more. It is a very useful tool in Python for web Scraping.\\nSometimes the Requests library is not enough to scrape a website. Some sites out there use JavaScript to serve content. For example, they might wait until you scroll down on the page or click a button before loading certain content.\\nOther sites may require',\n",
       " 'oftware created on top of this library can be a crawler, scraper, and data extractor or even all this together.\\nWith this Framework, you can create Spider that will crawl on web pages and scrape desired data from the web.\\n# To install â€” pip install\\nScrapy\\n# To import â€” import scrapy\\nHere is image Of\\nScrapy architecture, image borrowed from\\ndocumentaion\\n.\\nThis is the basic code for creating a spider with\\nScrapy\\n. There are tons of predefined class and methods and you just have to use them to crea',\n",
       " 'differ between how a human operates a website and how a bot does.\\nThis detection is not easy and requires a significant amount of programming work to accomplish correctly.\\nAuthentications\\nSometimes we need to scrape private data, which is available once you get authenticated on the website. Some times websites are simple straightforward just pass authentication in post headers and you are good to go but some are not that easy , they seek some hidden inputs or CSRF tokens or any other specific he',\n",
       " 'built to work for one site or can be configured to work with any website based on user requirements.\\nWeb scraping finds many uses both at a professional and personal level. Having different needs at different levels, some popular uses of web scraping are.\\nPotential need for web scrapping:\\nTo analyze customers feedback to understand the customerâ€™s perspective about their product that help companies to develop new strategies.\\nTo collect data to make services like content aggregator to serve conten',\n",
       " 'up\\nThe best use case for this library would be with static html pages.\\nHere are some basic operation examples -\\n3. LXML\\nThis is one of the best parsers for HTML and XML. It is used to ease the handling of XML and HTML files. It is widely used for its simplicity and extremely fast response. This library is very useful in web Scraping as this can easily parse the large HTML or XML files.\\nAmong all the Python web scraping libraries, weâ€™ve enjoyed using lxml the most. Itâ€™s straightforward, fast, and',\n",
       " 'his case you can use these captcha solving services . Note that some of these CAPTCHA solving services are fairly slow and expensive, so you may need to consider whether it is still economically viable to scrape sites that require continuous CAPTCHA solving over time. I am linking this\\narticle\\nif you come across this problem .\\nAvoid Honeypot Traps\\nThese are traps by some websites to detect any hacking activity . It can easily detect if the user is a bot . This problem is sometimes very hard to d',\n",
       " 'opyright , Violating Terms of Service (ToS) etc . Legality of Web Scraping is a grey area that tends to develop as time goes on. Although the web scrapers technically increase the speed up data surfing, loading, copying, and pasting web scraping is also the key culprit behind the increases cases of copyright violation, violated terms of use and other activities that are highly disruptive to a companyâ€™s business.\\nSHORT Word!\\nI look web scrapping as a tool . A tool that we should never misuse .\\nWe']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae99efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chunks = [\n",
    "    \"d click on links. Pretty cool, right?\\nIt also comes with Python bindings for controlling it right from your application. This makes it a breeze to integrate with your chosen parsing library.\\nYou can find Selenium\\nDocumentation\\nHere.\\n# To install â€” pip install\\nselenium\\n# To import â€” import selenium\\nHere are some basic operation example.\\nAdditional Resource :\\nhere are two articles where you can see some more in-depth operations the article is in two parts â€”\\nPART1\\nand\\nPART2\\n.\\n5. Scrapy\\nScrapy is a \",\n",
    "    \" and methods and you just have to use them to create your Spider. It is easy to create a web Spider with this package. Rather it is quite difficult for a beginner to create a fully functional web scraper.\\ntâ€™s a complete web scraping framework. That means you can use it to manage requests, preserve user sessions, follow redirects, and handle output pipelines.\\nIt also means you can swap out individual modules with other Python web scraping libraries. For instance, if you need to insert Selenium fo\",\n",
    "    \"fficial website to download the corresponding drivers. The most important thing is to install the corresponding Python selenium library, which is really not very convenient. In addition, if you want to do a large-scale deployment, some problems of the environment configuration also exist Itâ€™s a headache.\\nIn pypetter, there is actually a Chrome browser similar to Chrome browser behind it, which is performing some actions to render web pages.\\nResources :\\nThere is a very good article about this\\nher\",\n",
    "    \" is actually the implementation of the python version of puppeter, but it is not developed by Google. It is an unofficial version developed by an engineer from Japan based on some functions of puppeter. It uses chromium and it avoids tedious environment configurations.\\nWhen selenium is used, there is a trouble, that is, the configuration of the environment. You need to install relevant browsers, such as chrome, Firefox, etc., and then go to the official website to download the corresponding driv\",\n",
    "    \"s. For instance, if you need to insert Selenium for scraping dynamic web pages, you can do that.\\n(see examples in stack overflow\\nhere\\n)\\nSo if you need to reuse your crawler, scale it, manage complex data pipelines, or cook up some other sophisticated spider, then Scrapy was made for you.\\n6. Pyppeteer\\nPuppeter is a tool developed by Google based on node.js. With it, we can control some operations of Chrome browser through JavaScript. and pyppeteer is actually the implementation of the python vers\",\n",
    "]\n",
    "query = \"what is the link to this url and what is this article talking about?\"\n",
    "\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "full_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfdbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLaMA response: {'id': 'cmpl-a1321710-120c-4d0a-a434-91639ef4df1b', 'object': 'text_completion', 'created': 1746517511, 'model': './models/mistral.gguf', 'choices': [{'text': ' This article is talking about two different web scraping frameworks: Selenium and Scrapy. Selenium is a Python library that allows you to control web browsers and scrape data from websites. It comes with Python bindings and can be easily integrated with your chosen parsing library. Scrapy, on the other hand, is a complete web scraping framework that includes methods for managing requests, preserving user sessions, following redirects, and handling output pipelines. It allows you to swap out individual modules with other Python web scraping libraries and is developed by an engineer from Japan. Pyppeteer is an unofficial implementation of the python version of Puppeter that uses Chromium and avoids tedious environment configurations.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 620, 'completion_tokens': 155, 'total_tokens': 775}}\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Users/sunnyraj/code_files/git_repos/AdvanceRagApp')\n",
    "from backend.llm_engine import get_response_from_llm\n",
    "# from config import settings\n",
    "# settings[\"llm\"][\"model_path\"] = './models/mistral.gguf'\n",
    "# settings[\"llm\"][\"context_window\"] = \n",
    "# response = get_response_from_llm(full_prompt)\n",
    "response = get_response_from_llm(prompt=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f309cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article is talking about two different web scraping frameworks: Selenium and Scrapy. Selenium is a Python library that allows you to control web browsers and scrape data from websites. It comes with Python bindings and can be easily integrated with your chosen parsing library. Scrapy, on the other hand, is a complete web scraping framework that includes methods for managing requests, preserving user sessions, following redirects, and handling output pipelines. It allows you to swap out individual modules with other Python web scraping libraries and is developed by an engineer from Japan. Pyppeteer is an unofficial implementation of the python version of Puppeter that uses Chromium and avoids tedious environment configurations.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a5eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res = {\n",
    "    \"id\": \"cmpl-a1321710-120c-4d0a-a434-91639ef4df1b\",\n",
    "    \"object\": \"text_completion\",\n",
    "    \"created\": 1746517511,\n",
    "    \"model\": \"./models/mistral.gguf\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"text\": \" This article is talking about two different web scraping frameworks: Selenium and Scrapy. Selenium is a Python library that allows you to control web browsers and scrape data from websites. It comes with Python bindings and can be easily integrated with your chosen parsing library. Scrapy, on the other hand, is a complete web scraping framework that includes methods for managing requests, preserving user sessions, following redirects, and handling output pipelines. It allows you to swap out individual modules with other Python web scraping libraries and is developed by an engineer from Japan. Pyppeteer is an unofficial implementation of the python version of Puppeter that uses Chromium and avoids tedious environment configurations.\",\n",
    "            \"index\": 0,\n",
    "            \"logprobs\": None,\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 620, \"completion_tokens\": 155, \"total_tokens\": 775},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a828725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
