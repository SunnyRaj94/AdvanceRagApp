{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98e651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/sunnyraj/code_files/git_repos/AdvanceRagApp')\n",
    "\n",
    "from backend.llm_engine import get_response_from_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7307519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLMs, RAG stands for \"Risk, Analysis, and Governance.\" It is a framework used to assess and manage risks associated with large-scale machine learning models. The purpose of RAG is to provide a structured approach for identifying, evaluating, and mitigating risks that may arise from the use of these models.\n",
      "\n",
      "The RAG framework typically involves three stages:\n",
      "\n",
      "1. Risk: Identifying potential risks associated with the model, such as bias, data privacy, security, ethical concerns, and legal implications.\n",
      "2. Analysis: Evaluating the likelihood and potential impact of each identified risk. This includes assessing the model's performance, its interaction with the data and users, and any potential consequences of its decisions.\n",
      "3. Governance: Establishing policies, procedures, and controls to mitigate risks and ensure that the model is used responsibly and ethically. This includes implementing monitoring and auditing mechanisms, as well as providing transparency and accountability.\n",
      "\n",
      "The RAG framework is designed to help organizations manage the complex risks associated with large-scale machine learning models and ensure that these models are used in a responsible and ethical manner.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What is the purpose of RAG in LLMs? [/INST]\"\n",
    "\n",
    "print(get_response_from_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67b5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunnyraj/miniconda3/envs/rag_app/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Ingesting document: https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation\n",
      "âœ… Extracted 26576 characters\n",
      "âœ… Chunked into 60 pieces\n",
      "âœ… Created embeddings of shape (60, 384)\n",
      "âœ… Stored 60 chunks in vector store\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from backend.rag.ingest import ingest_document\n",
    "\n",
    "# From file\n",
    "# ingest_document(\"sample_docs/sample.pdf\")\n",
    "\n",
    "# From web\n",
    "ingest_document(\"https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
