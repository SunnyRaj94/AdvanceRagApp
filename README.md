# ğŸ§  docudialogue -> Local RAG App with context (docs/links etc)

A powerful **Retrieval-Augmented Generation (RAG)** application with:

* âœ… Support for **local LLMs** (via `llama.cpp`) and local embeddings
* ğŸ“„ Seamless ingestion of documents and URLs
* ğŸ’¬ Conversational chat with **context memory**
* ğŸ–¥ï¸ A clean and responsive **UI**
* âš™ï¸ Built to run efficiently on **MacBook Air M4** with full local control
* ğŸ›¡ï¸ **Privacy-first design** â€“ no data ever leaves your machine
* â˜ï¸ **Cloud-ready architecture** â€“ easily deployable on any cloud provider or on-prem server
---

## ğŸš€ Features

- ğŸ” Ask questions over uploaded **PDF, TXT, DOCX, CSV** files or **URLs**
- ğŸ§  Maintain chat context (session-aware)
- ğŸ“ View ingested files/URLs in the UI
- ğŸ§¬ FAISS-based vector store
- ğŸ’¾ Persisted metadata in `metadata.json`
- ğŸ§° Switch between **local** and **API-based** models (LLM + embedding)
- ğŸ¨ Modern React UI with real-time response loading

---

## ğŸ–¥ï¸ Tech Stack

- **Frontend**: HTML, CSS, JS
- **Backend**: Python (Flask or FastAPI recommended)
- **Database**: MongoDB (local or Atlas)
- **Embeddings & LLM**: Local embedding model + local LLM
- **Platform**: macOS (MacBook Air M4)

---


## ğŸ–¼ï¸ Screenshots

### â³ Loading State
![Loading](assets/loading.png) <!-- Replace with actual file path -->

---

### âœ… Response UI
![Response](assets/response.png) <!-- Replace with actual file path -->

---

## ğŸ› ï¸ Setup Instructions (MacBook Air M4)

> âœ… Tested on macOS Sonoma (ARM64)

### 1. Clone the repo

```bash
git clone https://github.com/sunnyraj94/localrag-ai.git
cd localrag-ai
````

### 2. Create and activate virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Python dependencies

```bash
pip install -r requirements.txt
```

> Includes: `fastapi`, `uvicorn`, `faiss-cpu`, `pydantic`, `transformers`, `sentence-transformers`, etc.

---

## âš™ï¸ LLM & Embedding Configuration

Edit the `settings.yaml` to change model types:

```yaml
llm:
  model_type: "llama-cpp"         # Options: llama-cpp, openai, huggingface
  model_path: "./models/mistral.gguf"
  ...

embedding:
  type: local                     # Options: local, huggingface, openai
  model_name: ./models/all-MiniLM-L6-v2
```

---

### ğŸ”» Download Local Models (Required)

#### 1. All-MiniLM-L6-v2 (for embedding)

```bash
# Run this from project root
mkdir -p models && cd models
git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
```

#### 2. Mistral GGUF model (for llama-cpp)

```bash
# Optional: If using llama-cpp as LLM
curl -L -o mistral.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

Update path in `settings.yaml`:

```yaml
llm:
  model_path: "./models/mistral.gguf"
```

---

## ğŸ“¦ Run Backend

```bash
uvicorn backend.main:app --reload
```

> ğŸ“ Accessible at: [http://localhost:8000](http://localhost:8000)

---

## ğŸ“„ Ingest Documents/Links via API

### Upload a file

```bash
curl -X POST http://localhost:8000/ingest/file \
  -F 'file=@/path/to/your/file.pdf'
```

### Ingest a URL

```bash
curl -X POST http://localhost:8000/ingest/link \
  -F 'url=https://example.com/some-article'
```

---

## ğŸ’¬ Query via Chat API

```bash
curl -X POST http://localhost:8000/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "query": "Summarize the PDF",
    "top_k": 5,
    "source": "all",
    "file_id": null,
    "link_id": null
  }'
```

Response :

```json
{
  "response": "Here's the summary based on the uploaded content..."
}
```

---

## ğŸ“ View Ingested Data

### Files

```bash
curl http://localhost:8000/context/files
```

### Links

```bash
curl http://localhost:8000/context/links
```
---

## ğŸ§° Project Structure

```
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py                     # Entry point (optional if using FastAPI routing)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml           # App, model, embedding config
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py                  # FastAPI router
â”‚   â”œâ”€â”€ auth.py                 # Login/session support
â”‚   â”œâ”€â”€ storage.py              # Local storage manager
â”‚   â”œâ”€â”€ llm_engine.py           # Handles LLM selection
â”‚   â”œâ”€â”€ rag_pipeline.py         # Orchestrates full RAG pipeline
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ ingest.py           # File/URL ingestion logic
â”‚       â”œâ”€â”€ query.py            # Query handling logic
â”‚       â”œâ”€â”€ processor.py        # Text processing, chunking
â”‚       â”œâ”€â”€ vector_store.py     # FAISS vector store handling
â”‚       â”œâ”€â”€ embedder.py         # Embedding logic
â”‚       â”œâ”€â”€ logger.py           # Metadata + query logging
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ ui.py                   # UI runner (if Flask/fastapi templates)
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ login.html
â”‚   â”‚   â”œâ”€â”€ upload.html
â”‚   â”‚   â””â”€â”€ chat.html
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ js/
â”‚           â””â”€â”€ app.js
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral.gguf            # LLM in GGUF format (llama.cpp)
â”‚   â””â”€â”€ all-MiniLM-L6-v2/       # Local embedding model
â”‚       â”œâ”€â”€ *.json / *.bin / *.onnx / *.xml
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ app.log                 # Application logs
â”‚   â””â”€â”€ metadata.json           # Ingested file/link metadata
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test.ipynb              # Test notebook
â”‚   â””â”€â”€ vector_store/
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ meta.pkl

```

---

## ğŸ”§ Customization

### Change LLM Backend

In `settings.yaml`, switch between:

* ğŸ§  **Local**: llama-cpp with `.gguf` file
* ğŸ”— **OpenAI**: `gpt-3.5-turbo`
* ğŸ¤— **Hugging Face**: `mistralai/Mistral-7B-Instruct-v0.1`

### Change Embedding Backend

Use:

* `local` (MiniLM, via SentenceTransformers)
* `huggingface` (cloud-hosted)
* `openai` (if needed)

---

## ğŸ“Œ TODO

* ğŸŸ¢ Integration of mongodb for metadata store
* ğŸŸ¢ Save and fetch vectorstore embeddings from sql db
* ğŸŸ¢ Export full chat history
* ğŸŸ¢ Chat session restore on refresh
* ğŸŸ¢ Model hot-reload toggle in UI
* ğŸŸ¢ Deploy to cloud
* ğŸŸ¢ Testing with multiple models + embeddings (local or cloud )
---

## ğŸ§‘â€ğŸ’» Author

Crafted with â¤ï¸ on a MacBook Air M4
*React + FastAPI + FAISS + LLMs*

---

## ğŸ“„ License

MIT License â€” Use it, fork it, build cool stuff!
